{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65ed242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "import pandas as pd\n",
    "from src.data import load_scaler\n",
    "from src.cluster import load_kmeans_model\n",
    "from src.eval import final_run\n",
    "from src.model import AutoEncoder\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17dd080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(cluster_no):\n",
    "    config_path = f'../models/ae_cluster_{cluster_no}/ae_cluster_{cluster_no}_config.txt'\n",
    "    config = {}\n",
    "    with open(config_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            if \":\" in line:\n",
    "                key, value = line.split(\":\", 1)\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "\n",
    "                # Try to convert to int/float, otherwise keep string\n",
    "                if value.isdigit():\n",
    "                    value = int(value)\n",
    "                else:\n",
    "                    try:\n",
    "                        value = float(value)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "\n",
    "                config[key] = value\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6fa36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating for cluster 0 with threshold 0.5\n",
      "Evaluating for cluster 1 with threshold 0.5\n",
      "Evaluating for cluster 2 with threshold 0.5\n",
      "Evaluating for cluster 3 with threshold 0.5\n",
      "  Cluster 3 - n=718\n",
      "    Accuracy : 0.9485\n",
      "    Precision: 0.7339\n",
      "    Recall   : 0.9091\n",
      "    F1       : 0.8122\n",
      "\n",
      "=== Overall metrics ===\n",
      "Accuracy   : 0.7439\n",
      "Error rate : 0.2561\n",
      "Precision  : 0.4307\n",
      "Recall     : 0.8720\n",
      "F1-score   : 0.5766\n",
      "Confusion matrix [ [TN FP], [FN TP] ]:\n",
      "[[1401  567]\n",
      " [  63  429]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/helgamariamagnusdottir/Documents/dtu/comp_tools/data_science/.venv/lib/python3.12/site-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.6.0 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/helgamariamagnusdottir/Documents/dtu/comp_tools/data_science/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "thresholds = [0.5, 0.5, 0.5, 0.5]   # threshold for each cluster\n",
    "tune_data_path='../data/tune_data.csv'\n",
    "model_paths = '../checkpoints/'\n",
    "#def final_run(thresholds=[0.5, 0.5, 0.5, 0.5], tune_data_path='../data/tune_data.csv'):\n",
    "\n",
    "# -- Load data --\n",
    "df = pd.read_csv(tune_data_path)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop(columns=[\"Class\"]).values\n",
    "y = df[\"Class\"].values \n",
    "\n",
    "# -- Scale & cluster data --\n",
    "scaler = load_scaler()\n",
    "X_scaled = scaler.transform(X)   \n",
    "\n",
    "kmeans_model = load_kmeans_model()\n",
    "cluster_labels = kmeans_model.predict(X_scaled)\n",
    "\n",
    "# Array for global predictions\n",
    "y_pred = np.zeros_like(y)\n",
    "cluster_metrics = {}\n",
    "\n",
    "for i, threshold in enumerate(thresholds):      \n",
    "    print(f\"Evaluating for cluster {i} with threshold {threshold}\")\n",
    "    \n",
    "    # mask for points in cluster i\n",
    "    mask = (cluster_labels == i)\n",
    "\n",
    "    X_cluster = X_scaled[mask]\n",
    "    y_cluster = y[mask]\n",
    "\n",
    "    # -- Load for this cluster --\n",
    "    weights_path = f'../models/ae_cluster_{i}/ae_cluster_{i}.pt'\n",
    "    weights = torch.load(weights_path, map_location=torch.device('cpu'))\n",
    "\n",
    "    # -- Load config --\n",
    "    cfg = load_config(cluster_no=i)\n",
    "\n",
    "    # Initialize model and load weights\n",
    "    model = AutoEncoder(in_dim=X_cluster.shape[1], hidden_units=cfg['hidden_dim'], latent_features=cfg['latent'], num_layers=cfg['num_layers'])\n",
    "    model.load_state_dict(weights)\n",
    "    model.eval()\n",
    "\n",
    "    # Forward pass and compute reconstruction error\n",
    "    X_cluster_t = torch.tensor(X_cluster, dtype=torch.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(X_cluster_t)          # out is a dict: {'z': ..., 'x_hat': ...}\n",
    "        reconstructed = out['x_hat']      # (batch_size, in_dim)\n",
    "        reconstruction_error = F.mse_loss(\n",
    "            reconstructed,\n",
    "            X_cluster_t,\n",
    "            reduction='none'\n",
    "        ).mean(dim=1)  # mean over features â†’ one error per sample\n",
    "    \n",
    "    # -- Thresholding --\n",
    "    reconstruction_error_np = reconstruction_error.cpu().numpy()\n",
    "    pred_cluster = (reconstruction_error_np > threshold).astype(int)\n",
    "\n",
    "    # Put the predictions back into the global array\n",
    "    y_pred[mask] = pred_cluster\n",
    "\n",
    "    acc_c = accuracy_score(y_cluster, pred_cluster)\n",
    "    prec_c = precision_score(y_cluster, pred_cluster, zero_division=0)\n",
    "    rec_c = recall_score(y_cluster, pred_cluster, zero_division=0)\n",
    "    f1_c = f1_score(y_cluster, pred_cluster, zero_division=0)\n",
    "\n",
    "    cluster_metrics[i] = {\n",
    "        \"accuracy\": acc_c,\n",
    "        \"precision\": prec_c,\n",
    "        \"recall\": rec_c,\n",
    "        \"f1\": f1_c,\n",
    "        \"n_samples\": len(y_cluster)\n",
    "    }\n",
    "\n",
    "    print(f\"  Cluster {i} - n={len(y_cluster)}\")\n",
    "    print(f\"    Accuracy : {acc_c:.4f}\")\n",
    "    print(f\"    Precision: {prec_c:.4f}\")\n",
    "    print(f\"    Recall   : {rec_c:.4f}\")\n",
    "    print(f\"    F1       : {f1_c:.4f}\")\n",
    "\n",
    "# --- Global metrics over all clusters ---\n",
    "acc = accuracy_score(y, y_pred)\n",
    "prec = precision_score(y, y_pred, zero_division=0)\n",
    "rec = recall_score(y, y_pred, zero_division=0)\n",
    "f1 = f1_score(y, y_pred, zero_division=0)\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "\n",
    "error_rate = 1.0 - acc\n",
    "\n",
    "print(\"\\n=== Overall metrics ===\")\n",
    "print(f\"Accuracy   : {acc:.4f}\")\n",
    "print(f\"Error rate : {error_rate:.4f}\")\n",
    "print(f\"Precision  : {prec:.4f}\")\n",
    "print(f\"Recall     : {rec:.4f}\")\n",
    "print(f\"F1-score   : {f1:.4f}\")\n",
    "print(\"Confusion matrix [ [TN FP], [FN TP] ]:\")\n",
    "print(cm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1b7bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "    model = Autoencoder(in_dim=X_scaled.shape[1], hidden_units=64, latent_features=2, num_layers=1)\n",
    "    model.load_state_dict(weights)\n",
    "\n",
    "    # Evaluate model on X_cluster\n",
    "    X_cluster_t = torch.tensor(X_cluster, dtype=torch.float32)\n",
    "    model.eval()\n",
    "\n",
    "    # No gradients needed\n",
    "    with torch.no_grad():\n",
    "        reconstructed = model(X_cluster_t)\n",
    "        \n",
    "\n",
    "\n",
    "# # Reconstruction error per sample\n",
    "    reconstruction_error = F.mse_loss(\n",
    "            reconstructed,                  \n",
    "            X_cluster_t,                  \n",
    "            reduction='none'\n",
    "        ).mean(dim=1)   # mean across features for each sample  \n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_t)\n",
    "        X_hat = outputs[\"x_hat\"]\n",
    "        errors = torch.mean((X_hat - X_t)**2, dim=1).cpu().numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
